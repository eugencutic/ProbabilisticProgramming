{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, lower=True, stops=True, emojis=True, \n",
    "               newlines=True, single_quotes=True, punctuation=True,\n",
    "               lemmatize=True, tokenize=True):\n",
    "    import re\n",
    "    \n",
    "    if lower:\n",
    "        def to_lower_text(text):\n",
    "            return \" \".join([word.lower() for word in text.split()])\n",
    "        \n",
    "        data = [to_lower_text(text) for text in data]\n",
    "    \n",
    "    if stops:\n",
    "        import nltk;\n",
    "        nltk.download('stopwords')\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = stopwords.words('english')\n",
    "        \n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "        \n",
    "        data  = [remove_stopwords(text) for text in data]\n",
    "        \n",
    "    if emojis:\n",
    "        def convert_emojis_and_emoticons_to_word(text):\n",
    "            for emote in UNICODE_EMO:\n",
    "                text = text.replace(emote, \n",
    "                                    \" \" + \n",
    "                                    \"_\".join(UNICODE_EMO[emote]\n",
    "                                    .replace(\",\",\"\")\n",
    "                                    .replace(\":\",\"\").split())\n",
    "                                    + \" \")\n",
    "            for emote in EMOTICONS:\n",
    "                text = re.sub(u'('+emote+')', \n",
    "                              \" \" + \"_\".join(EMOTICONS[emote].replace(\",\",\"\").split()) + \" \", \n",
    "                              text)\n",
    "\n",
    "            return text\n",
    "        data = [convert_emojis_and_emoticons_to_word(text) for text in data]\n",
    "        \n",
    "    if newlines:\n",
    "        data = [re.sub('\\s+', ' ', text) for text in data]\n",
    "        \n",
    "    if single_quotes:\n",
    "        data = [re.sub(\"\\'\", \"\", text) for text in data]\n",
    "        \n",
    "    if punctuation:\n",
    "        data = [re.sub('[^\\w\\s]','', text) for text in data]\n",
    "        \n",
    "    if lemmatize:\n",
    "        import spacy\n",
    "        import en_core_web_sm\n",
    "        nlp = en_core_web_sm.load()\n",
    "        allowed = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "        data_lemmatized = []\n",
    "        for text in data:\n",
    "            data_lemmatized.append(\" \".join([token.lemma_ for token in nlp(text) if token.pos_ in allowed]))\n",
    "        data = data_lemmatized\n",
    "        \n",
    "    if tokenize:\n",
    "        data = [text.split() for text in data]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this task we import Reuters articles from the nltk corpus and sampling 10 random articles which we then preprocess by lowercasing, eliminating anything which is not a word, eliminating stopwords, lemmatizing and converting each document string to a list of word strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\eugen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eugen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "reuters_training_docs = [reuters.raw(fileid) for fileid in reuters.fileids() if 'training' in fileid]\n",
    "data = random.sample(reuters_training_docs, 10)\n",
    "data = preprocess(data, emojis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitization\n",
    "The next block should be run only if a sanitization check is desired. It will overwrite the data variable so that the algorithm runs on dummy_docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_docs =  [[\"aaa\", \"bbb\", \"aaa\"],\n",
    "                [\"bbb\", \"aaa\", \"bbb\"],\n",
    "                [\"aaa\", \"bbb\", \"bbb\", \"aaa\"],\n",
    "                [\"uuu\", \"vvv\"],\n",
    "                [\"uuu\", \"vvv\", \"vvv\"],\n",
    "                [\"uuu\", \"vvv\", \"vvv\", \"uuu\"]]\n",
    "data = dummy_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation\n",
    "For representing the words we assign an id to each word incrementally and then transform the words in the documents to their corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [doc for doc in data if doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dummy_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for doc in data for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {word: idx for idx, word in enumerate(set(tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [[0 for j in range(len(data[i]))]\n",
    "                                    for i in range(len(data))]\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data_ids[i][j] = tokens[data[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of topics\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(data_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.ones(V)\n",
    "alphas = np.ones(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_m$ is the distribution of topics for document m (the probabilities of document m belonging to each topic). - chosen randomly ~ $Dirichlet(\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = pm.Container([pm.CompletedDirichlet('theta_%i'%i, pm.Dirichlet('prob_theta_%i'%i, theta=alphas)) for i in range(M)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi_k$ is the distribution of words in topic k (the probability of each word belonging to a given topic k) chosen randomly ~ $Dirichlet(\\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = pm.Container([pm.CompletedDirichlet('phi_%i'%i, pm.Dirichlet('prob_phi_%i'%i, theta=betas)) for i in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{m,n}$ is the topic distribution for the word at position n in document m (probabilities of that word belonging to each topic) ~ $Multinomial(\\theta_m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pm.Container(\n",
    "        [pm.Categorical('z_%i'%m, p=thetas[m], size=len(doc), value=np.random.randint(K, size=len(doc)))\n",
    "        for m, doc in enumerate(data_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{m, n}$ draws the actual word from the word distribution ($\\phi$) of the topic selected by $z_{m, n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pm.Container(\n",
    "        [pm.Categorical('w_%i_%i'%(m,i), p=pm.Lambda('phi_%i_%i'%(m,i), lambda z=z[m][i], phis=phis: phis[z]),\n",
    "                      value=data_ids[m][i], observed=True)\n",
    "        for m, doc in enumerate(data_ids) for i in range(len(doc))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 400000 of 400000 complete in 252.8 sec"
     ]
    }
   ],
   "source": [
    "model = pm.Model([thetas, phis, z, w])\n",
    "mcmc = pm.MCMC(model)\n",
    "mcmc.sample(400000, 100000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58698264 0.41301736]]\n",
      "[[0.07276805 0.92723195]]\n",
      "[[0.50812383 0.49187617]]\n",
      "[[0.55891186 0.44108814]]\n",
      "[[0.86030168 0.13969832]]\n",
      "[[0.17709168 0.82290832]]\n"
     ]
    }
   ],
   "source": [
    "## Probabilities of documents belonging to each topic\n",
    "for m in range(M):\n",
    "    print(mcmc.trace('theta_%i'%m)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "vvv 0.35338176319485654\n",
      "aaa 0.2721747008617533\n",
      "bbb 0.1927091449598438\n",
      "uuu 0.1817343909835463\n",
      "Topic 1:\n",
      "vvv 0.7476833240860259\n",
      "bbb 0.13176858584884615\n",
      "uuu 0.1103854632837634\n",
      "aaa 0.010162626781364592\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## Distribution in each topcic of the 5 most important words\n",
    "words = list(tokens.keys())\n",
    "for k in range(K):\n",
    "    word_distrib_in_topic = mcmc.trace('phi_%i'%k)[-1][0]\n",
    "    sorted_indexes = np.argsort(word_distrib_in_topic)[::-1]\n",
    "    sorted_words_distrib = np.sort(word_distrib_in_topic)[::-1]\n",
    "    most_important_indexes = sorted_indexes[:5]\n",
    "    most_important_words_distrib = sorted_words_distrib[:5]\n",
    "    print(\"Topic \" + str(k) + \":\")\n",
    "    for i in range(len(most_important_words_distrib)):\n",
    "        print(words[most_important_indexes[i]], str(most_important_words_distrib[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic based similarity between documents\n",
    "- Essentially, what needs to be done is to find a distance between documents, given their topic/topic distribution\n",
    "- In the most basic form this could be done by comparing their topics positionally. Having the same topic in the same position in the distribution should make them more similar. This has gaps and wouldn't be well defined, however.\n",
    "- Upon futher research, we find that there are well defined distance measures for probability distributions. For example the Kullback-Leibler (KL) distance.  \n",
    "The KL distance measures dissimilarity between to probabilities p and q according to the formula:  \n",
    "$KL(p, q) = \\sum_{i=1}^{T} p_i log \\frac{p_i}{q_i}$  \n",
    "Swapping $p$ and $q$ for $\\theta_m$ and $\\theta_n$, for 2 documents m and n, we can measure the similarity between 2 documents' topic distributions and so how similar the documents are in terms of their topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "n = 3\n",
    "theta_m = np.array(mcmc.trace('theta_%i'%m)[-1][0])\n",
    "theta_n = np.array(mcmc.trace('theta_%i'%n)[-1][0])\n",
    "\n",
    "KL = np.sum(theta_m * np.log(theta_m/theta_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7442661912041693"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05358214, 0.02297803, 0.11185776, 0.6623711 , 0.14921098])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05767892, 0.02716468, 0.62786727, 0.03111244, 0.25617669])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New documents\n",
    "For new documents there are 2 or 3 possible approaches\n",
    "- First, and probably most accurate, would be adding the new documents to the corpus and running the model again.\n",
    "- Second, which is more time and resource efficient, would be considering what the model knows as \"observed\" or \"fixed\" and only sampling for new documents, so topic assigments for old documents stay the same.  \n",
    "- However, we have $p_w^i$ the probability of word w being in topic i, and given we have N words in the new document, we can calculate the distribution for each topic, considering words are independent given a topic:  \n",
    "For each topic i, we compute $\\prod_{j=1}^{N} p^i_{w_j}$ OR in log: $\\sum_{j=1}^{N} log p^i_{w_j}$  \n",
    "This would ease computations even further, but there would be problems with out-of-vocabulary words.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
